
# 스크래핑의 목적 

논현동 아파트 매매 정보 수집 

대상 사이트 : 직방 

직방 매물정보를 수집해서 DB에 저장한다.



# Table 생성

CREATE TABLE apart_sales_info (
    id INT PRIMARY KEY,
    name VARCHAR(255),
    lat DOUBLE,
    lng DOUBLE,
    total_units INT,
    approval_date DATE,
    service_type VARCHAR(255),
    real_type VARCHAR(255),
    sido VARCHAR(255),
    gugun VARCHAR(255),
    dong VARCHAR(255),
    image_url VARCHAR(255),
    is_after_prefunding BOOLEAN,
    min_sales_price INT,
    max_sales_price INT,
    avg_sales_price INT,
    price_per_area INT
);


$ mkdir scraping
$ cd scraping/
$ python3 -m venv scr_env
$ source ./scr_env/bin/activate
$ pip install requests
$ pip install bs4
$ pip install mysql-connector-python
$ vi scraping.py

import requests
import mysql.connector
from datetime import datetime

def getToday():
    today = datetime.now()
    return today

# Fetching data from the API
api_url = "https://apis.zigbang.com/apt/locals/prices/on-danjis?minPynArea=10%ED%8F%89%EC%9D%B4%ED%95%98&maxPynArea=60%ED%8F%89%EB%8C%80%EC%9D%B4%EC%83%81&geohash=wydm
7"

response = requests.get(api_url)

if response.status_code == 200:
    data = response.json()  # Assuming the response is in JSON format

    print(data)
    apartments = data.get('filtered', [])  # Extracting apartments data

    # Connecting to MariaDB
    conn = mysql.connector.connect(
        host="tdddu.co.kr",
        user="pipelinedb",
        port="3306",
        password="Epdlxj12#",
        database="pipelinedb"
    )

    # Creating a cursor object using cursor() method
    cursor = conn.cursor()

    # Inserting data into MariaDB
    for json_data in apartments:
        try:
            # 전체 데이터를 찍는다.
            print(json_data)
            sales_data = json_data.get('price', {}).get('sales', {})

            insert_data = {
                'id': json_data['id'],
				'name': json_data['name'],
                'lat': json_data['lat'],
                'lng': json_data['lng'],
                'total_units': json_data.get('총세대수'),
                'approval_date': json_data.get('사용승인일'),
                'service_type': json_data.get('서비스구분'),
                'real_type': json_data.get('real_type'),
                'sido': json_data.get('sido'),
                'gugun': json_data.get('gugun'),
                'dong': json_data.get('dong'),
                'image_url': json_data.get('image'),
                'is_after_prefunding': json_data.get('is후분양'),
                'min_sales_price': sales_data.get('min'),
                'max_sales_price': sales_data.get('max'),
                'avg_sales_price': sales_data.get('avg'),
                'price_per_area': sales_data.get('perArea')
            }

            # SQL query to insert data into a table
            sql = """
                INSERT INTO apart_sales_info(
                    id, name, lat, lng, total_units, approval_date, service_type,
                    real_type, sido, gugun, dong, image_url, is_after_prefunding,
                    min_sales_price, max_sales_price, avg_sales_price, price_per_area
                ) VALUES (
                    %(id)s, %(name)s, %(lat)s, %(lng)s, %(total_units)s, %(approval_date)s,
                    %(service_type)s, %(real_type)s, %(sido)s, %(gugun)s, %(dong)s,
                    %(image_url)s, %(is_after_prefunding)s, %(min_sales_price)s,
                    %(max_sales_price)s, %(avg_sales_price)s, %(price_per_area)s
                )
            """

            # Execute the SQL command
            cursor.execute(sql, insert_data)
            conn.commit()

        except Exception as e:
            # Print the error message
            print(f"Error processing data: {e}")
            # Skip to the next iteration
            continue
# Close the database connection
    conn.close()

else:
    print("Failed to fetch data from the API")


$ python3 scraping.py

# aws 람다프로그램으로 변경

$ vi lambda_function.py

import requests
import mysql.connector
from datetime import datetime

def getToday():
    today = datetime.now()
    return today

def lambda_handler(event, context):
    # Fetching data from the API
    api_url = "https://apis.zigbang.com/apt/locals/prices/on-danjis?minPynArea=10%ED%8F%89%EC%9D%B4%ED%95%98&maxPynArea=60%ED%8F%89%EB%8C%80%EC%9D%B4%EC%83%81&geohash=wydm7"
    headers = {
        'User-Agent': 'Security-Token,ZigbangOsType,ZigbangDeviceId,ZigbangUserToken,User-Agent,X-Requested-With,Zigbang-UserToken'
    }

    response = requests.get(api_url)

    if response.status_code == 200:
        data = response.json()  # Assuming the response is in JSON format

        print(data)
        apartments = data.get('filtered', [])  # Extracting apartments data

        # Connecting to MariaDB
        conn = mysql.connector.connect(
            host="tripforu.co.kr",
            user="tripforu",
            port="3307",
            password="Dugodtkfkd04!",
            database="smartf"
        )

        # Creating a cursor object using cursor() method
        cursor = conn.cursor()

        # Inserting data into MariaDB
        for json_data in apartments:
            try:
                # 전체 데이터를 찍는다.
                print(json_data)
                sales_data = json_data.get('price', {}).get('sales', {})

                insert_data = {
                    'id': json_data['id'],
                    'name': json_data['name'],
                    'lat': json_data['lat'],
                    'lng': json_data['lng'],
                    'total_units': json_data.get('총세대수'),
                    'approval_date': json_data.get('사용승인일'),
                    'service_type': json_data.get('서비스구분'),
                    'real_type': json_data.get('real_type'),
                    'sido': json_data.get('sido'),
                    'gugun': json_data.get('gugun'),
                    'dong': json_data.get('dong'),
                    'image_url': json_data.get('image'),
                    'is_after_prefunding': json_data.get('is후분양'),
                    'min_sales_price': sales_data.get('min'),
                    'max_sales_price': sales_data.get('max'),
                    'avg_sales_price': sales_data.get('avg'),
                    'price_per_area': sales_data.get('perArea')
                }

                # SQL query to insert data into a table
                sql = """
                    INSERT INTO apart_sales_info (
                        id, name, lat, lng, total_units, approval_date, service_type,
                        real_type, sido, gugun, dong, image_url, is_after_prefunding,
                        min_sales_price, max_sales_price, avg_sales_price, price_per_area
                    ) VALUES (
                        %(id)s, %(name)s, %(lat)s, %(lng)s, %(total_units)s, %(approval_date)s,
                        %(service_type)s, %(real_type)s, %(sido)s, %(gugun)s, %(dong)s,
                        %(image_url)s, %(is_after_prefunding)s, %(min_sales_price)s,
                        %(max_sales_price)s, %(avg_sales_price)s, %(price_per_area)s
                    )
                """

                # Execute the SQL command
                cursor.execute(sql, insert_data)
                conn.commit()

            except Exception as e:
                # Print the error message
                print(f"Error processing data: {e}")
                # Skip to the next iteration
                continue

        # Close the database connection
        conn.close()

    else:
        print("Failed to fetch data from the API")

# aws 람다 패킹 
$ cd scr_env/lib/python3.9/site-packages/
$ zip -r ../../../../apart_cost_scraping.zip .
$ cd ../../../../
$ zip apart_cost_scraping.zip lambda_function.py
$ aws s3 cp apart_cost_scraping.zip s3://fc-storydata/

# aws console 에서 확인한다. 
